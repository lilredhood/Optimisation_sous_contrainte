{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "umlC6PGO65q5"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cwMkwmY57IH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On considère le problème de l’optimisation d’un portefeuille. On appelle J, la fonction définie sur $\\mathbf{R}$ par $J(x) = \\frac{1}{2}<x, Ax>_{\\mathbf{R}^n}$\n",
        "On désigne par $K$, l’ensemble des contraintes, soit $K = \\{ x \\in \\mathbf{R}^n \\vert <x, u> = 1, ~ <x, e> = r_0 \\}$. L’objectif est de résoudre numériquement le problème $\\inf_{x \\in K} J(x)$. Nous allons ré-écrire notre ensemble $K$ de la façon suivante : $K = \\{ x \\in Cx = f\\}$ avec : \n",
        " $C =$ \\begin{pmatrix} 1 & \\cdots & 1 \\\\ e_1 & \\cdots & e_n \\end{pmatrix}\n",
        "et $f = (1, r_0)^T$."
      ],
      "metadata": {
        "id": "x5y74NDi7MV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A l'aide du théorème des extrémas liés, nous avons les conditions d'optimalités suivante : $A x^* = \\lambda_1 u + \\lambda_2 e$ où $x^*$ est un minimiseur de $J$ et $\\lambda_1$ et $\\lambda_2$ sont des multiplicateurs de Lagrange. "
      ],
      "metadata": {
        "id": "Lm8OfPzM7fst"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ainsi on va se fixer deux multiplicateurs de lagrange $\\lambda^0_1$ et $\\lambda^0_2$ initial et tout d'abord résoudre le problème à l'itération $k$,$\\underset{x \\in \\mathbb{R}^n} {\\inf}  L(x, \\lambda^k )$  où $\\lambda^k$ est connue. On fera cela à l'aide d'un algorithme de gradient à pas constant. Ensuite nous devrons maximiser le problème, $\\underset{\\lambda \\in \\mathbb{R}^2}{\\sup}  L(x^k, \\lambda )$ où $x^k$ est la donnée calculée précédémment. Cela se fera aussi à l'aide d'un algorithme de gradient."
      ],
      "metadata": {
        "id": "TAwhdSh77_V2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1Bo73aKG8Ifd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On considère une fonction qui permet de générer la matrice $A$. De plus on prend, $r_0 = 2.5$ et $e_i = i$ pour tout $i$ allant de $1$ à $n$."
      ],
      "metadata": {
        "id": "-XueI-jk8JKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5\n",
        "r_0 = 2.5\n",
        "e = np.arange(1, n+1)\n",
        "u = np.ones(n)\n",
        "\n",
        "A = np.diag(e) # matrice diagonale pour l'instant.\n",
        "R = np.random.rand(n, n) # des nombres entre 0 et 1.\n",
        "A = A + 0.05 * np.dot(np.transpose(R),R) # produit scalaire entre deux matrices,  \n",
        "# correspond à un produit de somme sur le dernier axe de la première matrice et l'avant-dernier axe de \n",
        "# la deuxième. \n",
        "\n",
        "print(A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnrGMyL38L8F",
        "outputId": "c6b6d063-0964-49aa-b9ab-a97b1d74540c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.06574804 0.03573978 0.09217373 0.0493276  0.06878074]\n",
            " [0.03573978 2.02825208 0.04862192 0.04115198 0.03248549]\n",
            " [0.09217373 0.04862192 3.16142946 0.08056469 0.1221193 ]\n",
            " [0.0493276  0.04115198 0.08056469 4.09313573 0.07823195]\n",
            " [0.06878074 0.03248549 0.1221193  0.07823195 5.1161675 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La dernière ligne permet de rendre notre matrice à diagonale (strictement) dominante. Nous allons le vérifier numériquement."
      ],
      "metadata": {
        "id": "CdPF9Ma18j92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(n):\n",
        "    sum = 0\n",
        "    for j in range(n):\n",
        "      if(i != j):\n",
        "          sum += abs(A[i][j])\n",
        "\n",
        "    if(abs(A[i][i]) < sum):\n",
        "        print(\"Elle n'est pas à diagonale strictement dominante.\")\n",
        "        exit(0)\n",
        "\n",
        "print(\"Elle est à diagonale strictement dominante.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK2QwIrB8kcI",
        "outputId": "447c381e-8fea-4132-e05f-7edcbcd45201"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elle est à diagonale strictement dominante.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YVeVXx4i8tbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous voulons à présent résoudre le problème d'optimisation à l'aide de la méthode d'Uzawa. Nous aurons donc besoin d'implémenter la fonction $J$ et le lagrangien."
      ],
      "metadata": {
        "id": "YLUweEEP8sae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def J(x):\n",
        "    return 1/2* np.dot(x, A @ x)\n",
        "\n",
        "def grad_J(x):\n",
        "    return A @ x\n",
        "\n",
        "# Nos fonctions de contraintes h1 et h2 :\n",
        "def h1(x):\n",
        "    return np.inner(x, u) - 1\n",
        "\n",
        "def h2(x):\n",
        "    return np.dot(x, e) - r_0\n",
        "\n",
        "# Lagrangien et son gradient : \n",
        "def L(x, a, b): # a et b sont les multiplicateurs de Lagrange.\n",
        "    return J(x) + a * h1(x) + b * h2(x)\n",
        "\n",
        "def grad_L(x, a, b) :\n",
        "    return grad_J(x) + a * u + b * e"
      ],
      "metadata": {
        "id": "WYh61Xx98t--"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuite on implémente l'algorithme :"
      ],
      "metadata": {
        "id": "mGIa1IdO838P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nos deux pas pour les gradients à pas constants.\n",
        "rho_1 = 0.001\n",
        "rho_2 = 0.2\n",
        "\n",
        "# Fonction pour minimiser L(., a, b) sans contrainte.\n",
        "def minL(x_last, a, b, rho = rho_2) :\n",
        "\n",
        "  it = 0\n",
        "  itmax = 50000\n",
        "\n",
        "  x = x_last - rho * grad_L(x_last, a, b)\n",
        "  err = np.linalg.norm(grad_L(x, a, b)) # On veut le gradient nul.\n",
        "\n",
        "  while (err > 1e-8 and it < itmax):\n",
        "\n",
        "    x = x - rho * grad_L(x, a, b)\n",
        "    err = np.linalg.norm(grad_L(x, a, b))\n",
        "    it +=1 \n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "def Uzawa(a, b, rho = rho_1):\n",
        "\n",
        "  # Itération maximale.\n",
        "  itmax = 50000\n",
        "  it = 0\n",
        "\n",
        "  # Minimiser d'abord le problème sans contrainte.\n",
        "  x = minL(x_0, a, b)\n",
        "\n",
        "  # Puis maximiser le problème avec contrainte.\n",
        "  a1 = a + rho * h1(x)\n",
        "  b1 = b + rho * h2(x)\n",
        "\n",
        "  # Erreur.\n",
        "  err = np.linalg.norm(x-x_0) + np.abs(a1-a) + np.abs(b1-b)\n",
        "\n",
        "  a = a1\n",
        "  b = b1\n",
        "\n",
        "  while (err > 1e-8 and it < itmax):\n",
        "      \n",
        "    it += 1\n",
        "\n",
        "    x1 = minL(x, a, b)\n",
        "    a1 = a + rho * h1(x)\n",
        "    b1 = b + rho * h2(x)\n",
        "\n",
        "    err = np.linalg.norm(x - x1) + np.abs(a1 - a) + np.abs(b1 - b)\n",
        "\n",
        "    x = x1\n",
        "    a = a1\n",
        "    b = b1\n",
        "  return x, it, err"
      ],
      "metadata": {
        "id": "GL9PrRTW84PC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Il faut initialiser les multiplicateurs de lagrange à l'itération k = 0.\n",
        "a = 1\n",
        "b = 1\n",
        "\n",
        "n = 5\n",
        "\n",
        "# x_0 initial.\n",
        "x_0 = np.ones(n)\n",
        "\n",
        "\n",
        "x_appro, it, err = Uzawa(a, b)\n",
        "\n",
        "print(\"La solution est : \", x_appro)\n",
        "print(\"Le nombre d'itération obtenue : \", it) \n",
        "print(\"Bonne erreur ? : \", (err < 1e-8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fIHmWtL86q1",
        "outputId": "af40bd04-50ec-40fb-c545-b8fb1acc2ce7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La solution est :  [0.34180021 0.22413018 0.15698727 0.14640457 0.13067224]\n",
            "Le nombre d'itération obtenue :  21085\n",
            "Bonne erreur ? :  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un inconvéniant majeur qu'on peut noter pour l'algorithme d'Uzawa est le nombre d'itération très très grand. "
      ],
      "metadata": {
        "id": "w33r8Em79A5I"
      }
    }
  ]
}